{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-12-12T14:31:56.411097Z","iopub.status.busy":"2023-12-12T14:31:56.410609Z","iopub.status.idle":"2023-12-12T14:32:13.526519Z","shell.execute_reply":"2023-12-12T14:32:13.525607Z","shell.execute_reply.started":"2023-12-12T14:31:56.411065Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import torch\n","import numpy as np\n","import pandas as pd\n","from torch import Tensor, nn\n","from tqdm.notebook import tqdm\n","from torchaudio.models import Conformer\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import Trainer, TrainingArguments\n","device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-12-12T14:32:13.528901Z","iopub.status.busy":"2023-12-12T14:32:13.528219Z","iopub.status.idle":"2023-12-12T14:32:13.541822Z","shell.execute_reply":"2023-12-12T14:32:13.540588Z","shell.execute_reply.started":"2023-12-12T14:32:13.528869Z"},"trusted":true},"outputs":[],"source":["class conformer_encoder(nn.Module) :\n","    def __init__(self, kernel_size: int, num_channels: int, num_layers: int, feed_forward = 1024, num_heads = 16) :\n","        super().__init__()\n","\n","        self.embedding = nn.Sequential(nn.Embedding(457, num_channels//4), nn.ReLU(), nn.Linear(num_channels//4, num_channels//2), \n","                                        nn.ReLU(), nn.Linear(num_channels//2, num_channels))\n","        \n","        trans_encoder = nn.TransformerEncoderLayer(num_channels, num_heads, feed_forward)\n","        self.transformer_encoder = nn.TransformerEncoder(trans_encoder, num_layers)\n","        self.encoder =  Conformer(num_channels, num_heads, feed_forward, num_layers, kernel_size)     \n","        self.result = nn.Sequential(nn.Linear(num_channels*2, num_channels), nn.ReLU(), nn.Linear(num_channels, num_channels//2),\n","                                    nn.Linear(num_channels//2, num_channels//4), nn.ReLU(), nn.Linear(num_channels//4, num_channels//8), \n","                                    nn.ReLU(), nn.Linear(num_channels//8, 2))\n","        \n","    def forward(self, input_ids, length, mask) :\n","        \n","        mask = torch.unsqueeze(mask, dim=-1)\n","        max_len = torch.max(length)\n","        mask = mask[:, :max_len]\n","        input_ids = input_ids[:, :max_len]\n","        embedding = self.embedding(input_ids)*mask\n","        \n","        encoded, _ = self.encoder(embedding, length)\n","        trans_encoded = self.transformer_encoder(embedding*mask) + embedding\n","        result_input = torch.concat(((encoded + embedding)*mask, trans_encoded), dim=-1)\n","        output = self.result(result_input)*mask\n","\n","        return output"]},{"cell_type":"markdown","metadata":{},"source":["# DECODER FILE"]},{"cell_type":"markdown","metadata":{},"source":["The decoder will focus on one base at a time. It will use positional information to understand the base's location.\n","\n","Additionally, it will ignore bases that cannot react with the current base by masking irrelevant parts of the encoder's output."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class decoder(nn.Module) :\n","    def __init__(self, kernel_size: int, num_channels: int, num_layers: int, feed_forward = 1024, num_heads = 16) :\n","        super().__init__()\n","\n","        self.embedding = nn.Sequential(nn.Embedding(1, num_channels//4), nn.ReLU(), nn.Linear(num_channels//4, num_channels//2), \n","                                        nn.ReLU(), nn.Linear(num_channels//2, num_channels))\n","        \n","        trans_encoder = conformer_encoder(num_channels, num_heads, feed_forward)\n","        self.transformer_Decoder = nn.TransformerDecoder(trans_encoder, num_layers)    \n","        self.result = nn.Sequential(nn.Linear(num_channels*2, num_channels), nn.ReLU(), nn.Linear(num_channels, num_channels//2),\n","                                    nn.Linear(num_channels//2, num_channels//4), nn.ReLU(), nn.Linear(num_channels//4, num_channels//8), \n","                                    nn.ReLU(), nn.Linear(num_channels//8, 2))\n","        self.loss = nn.L1Loss()\n","        \n","    def forward(self, input_ids, length, mask, labels=None) :\n","        \n","        mask = torch.unsqueeze(mask, dim=-1)\n","        max_len = torch.max(length)\n","        mask = mask[:, :max_len]\n","        input_ids = input_ids[:, :max_len]\n","        embedding = self.embedding(input_ids)*mask\n","        \n","        encoded, _ = self.encoder(embedding, length)\n","        trans_encoded = self.transformer_encoder(embedding*mask) + embedding\n","        result_input = torch.concat(((encoded + embedding)*mask, trans_encoded), dim=-1)\n","        output = self.result(result_input)*mask\n","        \n","        if labels is not None :\n","            \n","            y = labels[:, :max_len]\n","            cover = y != 0\n","            output *= cover\n","            loss = torch.unsqueeze( self.loss(output, y), dim=0)\n","            return loss\n","        \n","        return output"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":6923401,"sourceId":51294,"sourceType":"competition"},{"datasetId":3719560,"sourceId":6822004,"sourceType":"datasetVersion"}],"dockerImageVersionId":30559,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}
