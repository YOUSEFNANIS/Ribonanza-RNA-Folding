{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6822004,"sourceType":"datasetVersion","datasetId":3719560}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nfrom torch import Tensor, nn\nfrom tqdm.notebook import tqdm\nfrom torchaudio.models import Conformer\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import Trainer, TrainingArguments\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-09-21T11:56:14.585163Z","iopub.execute_input":"2024-09-21T11:56:14.585427Z","iopub.status.idle":"2024-09-21T11:56:27.327932Z","shell.execute_reply.started":"2024-09-21T11:56:14.585387Z","shell.execute_reply":"2024-09-21T11:56:27.326912Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_parquet('/kaggle/input/stanford-ribonanza-rna-folding-converted/train_data.parquet')\ndf = df.replace(np.nan, 0.0)\ndf = df.drop([c for c in df.columns if 'reactivity_error' in c], axis='columns')\ndf['L'] = df.sequence.apply(len)\ndf = df[df.SN_filter == 1]\ndf = df.sort_values(by='signal_to_noise', ignore_index=True, ascending=False)\ndf = df.drop_duplicates(['sequence_id', 'experiment_type'])\ndf = df[df.sequence_id.duplicated(keep=False) == True].reset_index(drop=True)\ndf = df.drop(['dataset_name', 'reads', 'SN_filter', 'signal_to_noise'], axis='columns')","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:56:27.329680Z","iopub.execute_input":"2024-09-21T11:56:27.330278Z","iopub.status.idle":"2024-09-21T11:56:44.034414Z","shell.execute_reply.started":"2024-09-21T11:56:27.330250Z","shell.execute_reply":"2024-09-21T11:56:44.033375Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class RNA_Dataset(Dataset):\n    def __init__(self, df, size=206):\n        self.seq_map = {'A':1,'C':2,'G':3,'U':4}\n        self.size = size\n        \n        df_2A3 = df.loc[df.experiment_type=='2A3_MaP'].reset_index(drop=True)\n        df_DMS = df.loc[df.experiment_type=='DMS_MaP'].reset_index(drop=True)\n        \n        self.seq = df_2A3['sequence'].values\n        self.react_2A3 = torch.from_numpy(df_2A3[[c for c in df_2A3.columns if 'reactivity_0' in c]].values)\n        self.react_DMS = torch.from_numpy(df_DMS[[c for c in df_DMS.columns if 'reactivity_0' in c]].values)\n\n        df = None\n        df_2A3 = None\n        df_DMS = None\n\n    def __len__(self):\n        return len(self.seq)\n    \n    def __getitem__(self, idx):\n        seq = self.seq[idx]\n        length = len(seq)\n        seq = [self.seq_map[s] for s in seq]\n        seq = torch.Tensor(seq).to(torch.int32)\n        mask = torch.zeros(self.size, dtype=torch.bool)\n        mask[:len(seq)] = 1\n        \n        output = dict()\n        output['input_ids'] = nn.functional.pad(seq,(0,self.size-length))\n        output['length'] = length\n        output['mask'] = mask\n        output['labels'] = nn.functional.pad(torch.stack([self.react_DMS[idx], self.react_2A3[idx]],-1), (0, 0, 0, 2))\n        \n        return output","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:56:44.035656Z","iopub.execute_input":"2024-09-21T11:56:44.035951Z","iopub.status.idle":"2024-09-21T11:56:44.046936Z","shell.execute_reply.started":"2024-09-21T11:56:44.035926Z","shell.execute_reply":"2024-09-21T11:56:44.046047Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class RNA_testset(Dataset):\n    def __init__(self, df, size=457):\n        self.size = size\n        self.df = df\n        self.seq_map = {'A':1,'C':2,'G':3,'U':4}\n\n        self.seq = df['sequence'].values\n        self.id_min = df.id_min\n\n    def __len__(self):\n        return len(self.seq)\n    \n    def __getitem__(self, idx):\n        seq = self.seq[idx]\n        length = len(seq)\n        seq = [self.seq_map[s] for s in seq]\n        seq = torch.Tensor(seq).to(torch.int32)\n        mask = torch.zeros(self.size, dtype=torch.bool)\n        mask[:len(seq)] = 1\n        seq = nn.functional.pad(seq,(0,self.size-length))\n\n        return seq, length, mask, self.id_min.iloc[idx]","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:56:44.049263Z","iopub.execute_input":"2024-09-21T11:56:44.049647Z","iopub.status.idle":"2024-09-21T11:56:44.058868Z","shell.execute_reply.started":"2024-09-21T11:56:44.049615Z","shell.execute_reply":"2024-09-21T11:56:44.057982Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class conformer(nn.Module) :\n    def __init__(self, kernel_size: int, num_channels: int, num_layers: int, feed_forward = 1024, num_heads = 16) :\n        super().__init__()\n\n        self.postional_embedding = nn.Sequential(nn.Embedding(457, num_channels//4), nn.Sigmoid(), nn.Linear(num_channels//4, num_channels//2), nn.ReLU())        \n        self.base_embedding = nn.Sequential(nn.Embedding(5, num_channels//4), nn.Sigmoid(), nn.Linear(num_channels//4, num_channels//2), nn.ReLU())\n        \n        self.feed_forward = nn.Sequential(nn.Linear(num_channels, num_channels*2), nn.Sigmoid(), nn.Linear(num_channels*2, num_channels), nn.ReLU())\n        \n        self.encoder =  Conformer(num_channels, num_heads, feed_forward, num_layers, kernel_size)     \n        self.result = nn.Sequential(nn.Sigmoid(), nn.Linear(num_channels, num_channels//2), nn.Linear(num_channels//2, num_channels//4), \n                                    nn.ReLU(), nn.Linear(num_channels//4, num_channels//8), nn.ReLU(), nn.Linear(num_channels//8, 2), nn.ReLU())\n        self.loss = nn.L1Loss()\n        \n    def forward(self, input_ids, length, mask, labels=None) :\n\n        mask = torch.unsqueeze(mask, dim=-1)\n        max_len = torch.max(length)\n        mask = mask[:, :max_len]\n        input_ids = input_ids[:, :max_len]\n        \n        positional_embedding = self.postional_embedding(input_ids)*mask\n        base_embedding = self.base_embedding(input_ids)*mask\n        embedding = torch.concat((positional_embedding, base_embedding), -1)\n        feed_forward = self.feed_forward(embedding) + embedding\n        \n        encoded, _ = self.encoder(feed_forward, length)\n        output = self.result(encoded)*mask\n        \n        if labels is not None :\n\n            y = labels[:, :max_len]\n            cover = y != 0\n            output *= cover\n            loss = torch.unsqueeze( self.loss(output, y), dim=0)\n            return loss\n        \n        return output","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:56:44.060281Z","iopub.execute_input":"2024-09-21T11:56:44.060564Z","iopub.status.idle":"2024-09-21T11:56:44.073177Z","shell.execute_reply.started":"2024-09-21T11:56:44.060523Z","shell.execute_reply":"2024-09-21T11:56:44.072334Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/\",\n    report_to = 'none',\n    lr_scheduler_type='cosine',\n    per_device_train_batch_size=32,\n    gradient_accumulation_steps=1,\n    learning_rate=5e-5,\n    weight_decay = 1e-5,\n    warmup_steps=50,\n    num_train_epochs=20,\n    save_strategy='epoch',\n    fp16=True,\n    logging_steps=10,\n    save_steps = 1.0,\n    torch_compile=True\n)\ndataset = RNA_Dataset(df, size=206)\nmodel = conformer(kernel_size = 7, num_channels = 256, num_layers =20, num_heads = 16, feed_forward = 1024)\ntrainer = Trainer(model = model, args=training_args, train_dataset = dataset)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:56:44.074376Z","iopub.execute_input":"2024-09-21T11:56:44.075102Z","iopub.status.idle":"2024-09-21T11:56:45.099578Z","shell.execute_reply.started":"2024-09-21T11:56:44.075068Z","shell.execute_reply":"2024-09-21T11:56:45.098769Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"The speedups for torchdynamo mostly come wih GPU Ampere or higher and which is not detected here.\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:56:45.100671Z","iopub.execute_input":"2024-09-21T11:56:45.100958Z","iopub.status.idle":"2024-09-21T11:57:42.161766Z","shell.execute_reply.started":"2024-09-21T11:56:45.100932Z","shell.execute_reply":"2024-09-21T11:57:42.160358Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='77' max='52500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   77/52500 00:51 < 10:00:10, 1.46 it/s, Epoch 0.03/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.216800</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.211400</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.204300</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.196200</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.200700</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.182100</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.183000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1553\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1835\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1835\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1838\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1839\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1840\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1841\u001b[0m ):\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1843\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2690\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2688\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2689\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2690\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2692\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:1921\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1921\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1923\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}